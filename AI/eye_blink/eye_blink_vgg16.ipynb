{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54332 images belonging to 2 classes.\n",
      "Found 13585 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = 'C:/Users/s_csmscox/jupyterSave/eye_blink/train'\n",
    "validation_dir = 'C:/Users/s_csmscox/jupyterSave/eye_blink/valid'\n",
    "\n",
    "batch_size = 50\n",
    "img_size = 64\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# train_datagen = ImageDataGenerator(rescale=1/255,\n",
    "#                                    rotation_range=20,  # 지정된 각도 범위내에서 임의로 원본 이미지를 회전\n",
    "#                                    width_shift_range=0.1,\n",
    "#                                    height_shift_range=0.1,\n",
    "#                                    zoom_range=0.1,    #  1-수치 혹은 1+수치만큼 확대 및 축소\n",
    "#                                    horizontal_flip=True,\n",
    "#                                    vertical_flip=True)\n",
    "\n",
    "\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_size,img_size),                      \n",
    "    batch_size=batch_size,     \n",
    "    class_mode='categorical'                    \n",
    ")\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_size,img_size),                       \n",
    "    batch_size=batch_size,    \n",
    "    class_mode='categorical'                    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 2, 2, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4096)              8392704   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 33,598,274\n",
      "Trainable params: 18,883,586\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.applications import VGG16\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# 모델 구축\n",
    "\n",
    "# vgg16 모델 불러오기\n",
    "pre_trained_vgg = VGG16(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
    "pre_trained_vgg.trainable = False\n",
    "pre_trained_vgg.summary()\n",
    "\n",
    "# vgg16 모델에 덧붙이기\n",
    "model = Sequential()\n",
    "model.add(pre_trained_vgg)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu', kernel_initializer='he_normal'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(2048, activation='relu', kernel_initializer='he_normal'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(1024, activation='relu', kernel_initializer='he_normal'))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-5), loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1087/1086 [==============================] - 218s 201ms/step - loss: 0.2857 - accuracy: 0.8871 - val_loss: 0.1829 - val_accuracy: 0.9382\n",
      "Epoch 2/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.1643 - accuracy: 0.9410 - val_loss: 0.1371 - val_accuracy: 0.9522\n",
      "Epoch 3/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.1340 - accuracy: 0.9519 - val_loss: 0.1361 - val_accuracy: 0.9480\n",
      "Epoch 4/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.1185 - accuracy: 0.9572 - val_loss: 0.1070 - val_accuracy: 0.9619\n",
      "Epoch 5/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.1047 - accuracy: 0.9623 - val_loss: 0.1020 - val_accuracy: 0.9658\n",
      "Epoch 6/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0982 - accuracy: 0.9654 - val_loss: 0.0957 - val_accuracy: 0.9654\n",
      "Epoch 7/100\n",
      "1087/1086 [==============================] - 23s 21ms/step - loss: 0.0913 - accuracy: 0.9669 - val_loss: 0.0924 - val_accuracy: 0.9662\n",
      "Epoch 8/100\n",
      "1087/1086 [==============================] - 23s 21ms/step - loss: 0.0853 - accuracy: 0.9692 - val_loss: 0.0849 - val_accuracy: 0.9698\n",
      "Epoch 9/100\n",
      "1087/1086 [==============================] - 23s 21ms/step - loss: 0.0794 - accuracy: 0.9710 - val_loss: 0.0870 - val_accuracy: 0.9672\n",
      "Epoch 10/100\n",
      "1087/1086 [==============================] - 23s 21ms/step - loss: 0.0765 - accuracy: 0.9726 - val_loss: 0.0901 - val_accuracy: 0.9701\n",
      "Epoch 11/100\n",
      "1087/1086 [==============================] - 22s 21ms/step - loss: 0.0717 - accuracy: 0.9741 - val_loss: 0.0763 - val_accuracy: 0.9731\n",
      "Epoch 12/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0718 - accuracy: 0.9738 - val_loss: 0.0775 - val_accuracy: 0.9723\n",
      "Epoch 13/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0650 - accuracy: 0.9770 - val_loss: 0.0748 - val_accuracy: 0.9726\n",
      "Epoch 14/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0614 - accuracy: 0.9775 - val_loss: 0.0725 - val_accuracy: 0.9759\n",
      "Epoch 15/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0584 - accuracy: 0.9788 - val_loss: 0.0794 - val_accuracy: 0.9711\n",
      "Epoch 16/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0598 - accuracy: 0.9784 - val_loss: 0.0793 - val_accuracy: 0.9720\n",
      "Epoch 17/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0552 - accuracy: 0.9798 - val_loss: 0.0677 - val_accuracy: 0.9770\n",
      "Epoch 18/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0561 - accuracy: 0.9791 - val_loss: 0.0688 - val_accuracy: 0.9765\n",
      "Epoch 19/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0510 - accuracy: 0.9815 - val_loss: 0.0834 - val_accuracy: 0.9703\n",
      "Epoch 20/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0523 - accuracy: 0.9806 - val_loss: 0.0691 - val_accuracy: 0.9774\n",
      "Epoch 21/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0497 - accuracy: 0.9810 - val_loss: 0.0709 - val_accuracy: 0.9762\n",
      "Epoch 22/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0460 - accuracy: 0.9832 - val_loss: 0.0826 - val_accuracy: 0.9726\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(patience = 5) # 조기종료 콜백함수 정의\n",
    "\n",
    "# 데이터 개수 / batch_size\n",
    "steps_per_epoch = 54332 / batch_size\n",
    "val_steps = 13585 / batch_size\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=100,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=val_steps,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Fine Tuning\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "img_size = 64\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 5) # 조기종료 콜백함수 정의\n",
    "\n",
    "# 데이터 개수 / batch_size\n",
    "steps_per_epoch = 54332 / batch_size\n",
    "val_steps = 13585 / batch_size\n",
    "\n",
    "# InceptionResNetV2 모델 불러오기\n",
    "pre_trained_vgg = VGG16(weights='imagenet', include_top=False, input_shape=(img_size, img_size, 3))\n",
    "pre_trained_vgg.trainable = False\n",
    "\n",
    "set_trainable = False\n",
    "for layer in pre_trained_vgg.layers:\n",
    "    if layer.name == 'block4_conv1 ':\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "# 일반적으로 learning_rate를 더 작게 설정\n",
    "model.compile(optimizer=Adam(learning_rate=1e-7), loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1087/1086 [==============================] - 28s 25ms/step - loss: 0.0363 - accuracy: 0.9870 - val_loss: 0.0627 - val_accuracy: 0.9782\n",
      "Epoch 2/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0344 - accuracy: 0.9879 - val_loss: 0.0619 - val_accuracy: 0.9789\n",
      "Epoch 3/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0337 - accuracy: 0.9883 - val_loss: 0.0615 - val_accuracy: 0.9789\n",
      "Epoch 4/100\n",
      "1087/1086 [==============================] - 23s 21ms/step - loss: 0.0333 - accuracy: 0.9884 - val_loss: 0.0614 - val_accuracy: 0.9789\n",
      "Epoch 5/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0330 - accuracy: 0.9885 - val_loss: 0.0612 - val_accuracy: 0.9792\n",
      "Epoch 6/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0328 - accuracy: 0.9887 - val_loss: 0.0610 - val_accuracy: 0.9792\n",
      "Epoch 7/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0326 - accuracy: 0.9887 - val_loss: 0.0609 - val_accuracy: 0.9794\n",
      "Epoch 8/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0325 - accuracy: 0.9888 - val_loss: 0.0609 - val_accuracy: 0.9792\n",
      "Epoch 9/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0324 - accuracy: 0.9888 - val_loss: 0.0613 - val_accuracy: 0.9789\n",
      "Epoch 10/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0323 - accuracy: 0.9888 - val_loss: 0.0610 - val_accuracy: 0.9791\n",
      "Epoch 11/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0321 - accuracy: 0.9888 - val_loss: 0.0609 - val_accuracy: 0.9791\n",
      "Epoch 12/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0321 - accuracy: 0.9889 - val_loss: 0.0610 - val_accuracy: 0.9792\n",
      "Epoch 13/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0320 - accuracy: 0.9889 - val_loss: 0.0610 - val_accuracy: 0.9792\n",
      "Epoch 14/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0319 - accuracy: 0.9888 - val_loss: 0.0607 - val_accuracy: 0.9795\n",
      "Epoch 15/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0319 - accuracy: 0.9887 - val_loss: 0.0606 - val_accuracy: 0.9794\n",
      "Epoch 16/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0317 - accuracy: 0.9890 - val_loss: 0.0610 - val_accuracy: 0.9787\n",
      "Epoch 17/100\n",
      "1087/1086 [==============================] - 22s 21ms/step - loss: 0.0317 - accuracy: 0.9888 - val_loss: 0.0608 - val_accuracy: 0.9794\n",
      "Epoch 18/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0316 - accuracy: 0.9890 - val_loss: 0.0605 - val_accuracy: 0.9796\n",
      "Epoch 19/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0316 - accuracy: 0.9891 - val_loss: 0.0605 - val_accuracy: 0.9799\n",
      "Epoch 20/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0315 - accuracy: 0.9891 - val_loss: 0.0608 - val_accuracy: 0.9791\n",
      "Epoch 21/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0315 - accuracy: 0.9890 - val_loss: 0.0605 - val_accuracy: 0.9796\n",
      "Epoch 22/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0314 - accuracy: 0.9890 - val_loss: 0.0605 - val_accuracy: 0.9799\n",
      "Epoch 23/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0313 - accuracy: 0.9891 - val_loss: 0.0605 - val_accuracy: 0.9796\n",
      "Epoch 24/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0313 - accuracy: 0.9891 - val_loss: 0.0605 - val_accuracy: 0.9798\n",
      "Epoch 25/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0313 - accuracy: 0.9891 - val_loss: 0.0604 - val_accuracy: 0.9799\n",
      "Epoch 26/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0312 - accuracy: 0.9892 - val_loss: 0.0604 - val_accuracy: 0.9800\n",
      "Epoch 27/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0312 - accuracy: 0.9891 - val_loss: 0.0606 - val_accuracy: 0.9795\n",
      "Epoch 28/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0311 - accuracy: 0.9891 - val_loss: 0.0604 - val_accuracy: 0.9798\n",
      "Epoch 29/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0310 - accuracy: 0.9890 - val_loss: 0.0605 - val_accuracy: 0.9796\n",
      "Epoch 30/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0310 - accuracy: 0.9893 - val_loss: 0.0609 - val_accuracy: 0.9792\n",
      "Epoch 31/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0310 - accuracy: 0.9893 - val_loss: 0.0604 - val_accuracy: 0.9801\n",
      "Epoch 32/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0309 - accuracy: 0.9894 - val_loss: 0.0603 - val_accuracy: 0.9797\n",
      "Epoch 33/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0308 - accuracy: 0.9891 - val_loss: 0.0604 - val_accuracy: 0.9798\n",
      "Epoch 34/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0309 - accuracy: 0.9893 - val_loss: 0.0604 - val_accuracy: 0.9798\n",
      "Epoch 35/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0308 - accuracy: 0.9894 - val_loss: 0.0603 - val_accuracy: 0.9801\n",
      "Epoch 36/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0307 - accuracy: 0.9894 - val_loss: 0.0609 - val_accuracy: 0.9793\n",
      "Epoch 37/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0307 - accuracy: 0.9893 - val_loss: 0.0602 - val_accuracy: 0.9801\n",
      "Epoch 38/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0307 - accuracy: 0.9892 - val_loss: 0.0602 - val_accuracy: 0.9799\n",
      "Epoch 39/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0306 - accuracy: 0.9893 - val_loss: 0.0604 - val_accuracy: 0.9798\n",
      "Epoch 40/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0306 - accuracy: 0.9891 - val_loss: 0.0603 - val_accuracy: 0.9801\n",
      "Epoch 41/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0306 - accuracy: 0.9895 - val_loss: 0.0602 - val_accuracy: 0.9804\n",
      "Epoch 42/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0305 - accuracy: 0.9894 - val_loss: 0.0603 - val_accuracy: 0.9801\n",
      "Epoch 43/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0305 - accuracy: 0.9895 - val_loss: 0.0602 - val_accuracy: 0.9803\n",
      "Epoch 44/100\n",
      "1087/1086 [==============================] - 22s 21ms/step - loss: 0.0304 - accuracy: 0.9894 - val_loss: 0.0604 - val_accuracy: 0.9799\n",
      "Epoch 45/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0304 - accuracy: 0.9893 - val_loss: 0.0603 - val_accuracy: 0.9799\n",
      "Epoch 46/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0304 - accuracy: 0.9894 - val_loss: 0.0602 - val_accuracy: 0.9802\n",
      "Epoch 47/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0303 - accuracy: 0.9894 - val_loss: 0.0603 - val_accuracy: 0.9799\n",
      "Epoch 48/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0303 - accuracy: 0.9896 - val_loss: 0.0604 - val_accuracy: 0.9798\n",
      "Epoch 49/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0303 - accuracy: 0.9895 - val_loss: 0.0603 - val_accuracy: 0.9798\n",
      "Epoch 50/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0303 - accuracy: 0.9895 - val_loss: 0.0601 - val_accuracy: 0.9803\n",
      "Epoch 51/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0302 - accuracy: 0.9894 - val_loss: 0.0602 - val_accuracy: 0.9802\n",
      "Epoch 52/100\n",
      "1087/1086 [==============================] - 22s 21ms/step - loss: 0.0302 - accuracy: 0.9894 - val_loss: 0.0608 - val_accuracy: 0.9793\n",
      "Epoch 53/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0302 - accuracy: 0.9895 - val_loss: 0.0605 - val_accuracy: 0.9796\n",
      "Epoch 54/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0301 - accuracy: 0.9893 - val_loss: 0.0602 - val_accuracy: 0.9801\n",
      "Epoch 55/100\n",
      "1087/1086 [==============================] - 22s 20ms/step - loss: 0.0301 - accuracy: 0.9896 - val_loss: 0.0602 - val_accuracy: 0.9804\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=100,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=val_steps,\n",
    "                    callbacks=[early_stopping],\n",
    "                    verbose=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('C:/Users/s_csmscox/jupyterSave/eye_blink/eye_blink_vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('C:/Users/s_csmscox/jupyterSave/eye_blink/eye_blink_vgg16.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('C:/Users/s_csmscox/anaconda3/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier('C:/Users/s_csmscox/anaconda3/Lib/site-packages/cv2/data/haarcascade_eye.xml')\n",
    "img = cv2.imread('./tenet.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    roi_gray = gray[y:y+h, x:x+w]\n",
    "    roi_color = img[y:y+h, x:x+w]\n",
    "    eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "        img_trim = img[y+ey:y+ey+eh, x+ex:x+ex+ew]\n",
    "        \n",
    "        \n",
    "plt.imshow(img_trim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img = img.resize((128,128))\n",
    "        \n",
    "img = np.asarray(img)\n",
    "img = img.reshape(128,128,3)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n",
    "img = img/255\n",
    "\n",
    "img = img.reshape(1,128,128,3)\n",
    "pred = model.predict(img)\n",
    "\n",
    "for i in pred:\n",
    "    m = i.max()\n",
    "    for j in range(2):\n",
    "        if i[j] == m:\n",
    "            if j == 0:\n",
    "                print(\"예측 : 고양이\")\n",
    "            else:\n",
    "                print(\"예측 : 강아지\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
